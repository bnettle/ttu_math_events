In this talk, we propose a unified analysis of Bregman proximal first-order algorithms for convex minimization.  This flexible and versatile class of algorithms includes many well-known gradient-based schemes such as gradient descent, projected gradient descent, and proximal gradient descent.  This algorithmic class offers enormous potential to tackle large-scale optimization problems arising in data science and a variety of disciplines.  Our approach, which depends on the Fenchel conjugate, yields novel proofs of the convergence rates of the Bregman proximal subgradient and gradient algorithms, and a new accelerated Bregman proximal gradient algorithm.  We illustrate the effectiveness of Bregman proximal methods on two problems of great interest in data science, namely the D-optimal design and Poisson linear inverse problems.
