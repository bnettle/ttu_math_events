Bayesian methods provide a standard framework for statistical inference in which current "prior" beliefs about a population under study are combined with evidence provided by data to produce revised "posterior" beliefs about the population. &nbsp;As with all likelihood-based methods, Bayesian methods may present drawbacks stemming from model-misspecification and over-parametrization. &nbsp;A generalization of Bayesian methods, called Gibbs methods, link the data and population parameters of interest via a loss function rather than a likelihood, thereby avoiding these potential difficulties. &nbsp;At the same time, Gibbs methods retain the prior-to-posterior updating of beliefs. &nbsp;We will illustrate the advantages of Gibbs methods in examples, discuss the derivation of the Gibbs prior-to-posterior update, and highlight newly developed strategies to analyze the large-sample properties of Gibbs posterior distributions.