Deep learning has been successfully applied to many high-dimensional problems including computer vision, speech recognition, and numerical PDEs. In this talk, we first instroduce explicit error characterization of deep network approximation. Second, we present connections between deep network approximation, Monte Carlo sampling, random orthogonal projection, and Kolmogorovâ€“Arnold representation theorem on the curse of dimensionality. These connections leads to new error estimates for the approximation of multivariate functions by deep networks, for which the curse of the dimensionality is lessened.