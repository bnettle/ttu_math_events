Deep neural networks and other deep learning methods have very successfully been applied to the numerical approximation of high-dimensional nonlinear parabolic partial differential equations (PDEs). In particular, simulations indicate that algorithms based on deep learning overcome the curse of dimensionality in the numerical approximation of solutions of semilinear PDEs. For certain linear PDEs this has also been proved mathematically. However, proofs in the more general case have been difficult to rigorously formulate. To that end, we present a novel framework to alleviate this issue. In this talk, we will briefly introduce the concept of deep learning and then define an abstract framework which is amenable to proving theoretical results. We then prove some auxiliary results regarding the representation of particular approximations of a class of PDEs. In particular, we prove in the case of semilinear heat equations with gradient-independent nonlinearities that the numbers of parameters of the employed deep neural networks grow at most polynomially in both the PDE dimension and the reciprocal of the prescribed approximation accuracy. Our proof relies on recently introduced full history recursive multilevel Picard approximations of semilinear PDEs.